\section{Literature Review}

Rautaray, S. S., \& Agrawal, A. (2013) [1], focuses on developing a vision-based gesture recognition system that eliminates the need for physical input devices, allowing users to interact with digital objects using natural hand movements. The system employs computer vision algorithms for gesture detection, segmentation, tracking, and recognition, converting hand gestures into meaningful commands for real-time applications. The study highlights the limitations of traditional input methods, such as keyboards and mice, and emphasizes the advantages of markerless tracking systems in improving usability and immersion in dynamic environments. The proposed system contributes to the growing field of gesture-based interfaces.

Chowdhury and Haque (2013) [8], developed an animatronic hand controller that utilized flex sensors and servo motors, showcasing an innovative approach to robotic hand manipulation. The flex sensors played a crucial role in detecting finger bending motions, translating them into electrical signals that corresponded to hand gestures. These signals were then processed by an Arduino microcontroller, which controlled the servo motors responsible for mimicking natural finger movements. By integrating this sensor-motor system, the researchers enabled precise gesture-based control, allowing for applications in robotic prosthetics, remote-controlled manipulators, and assistive technology. Their work underscored the potential of Arduino-driven systems for creating cost-effective and accessible animatronic solutions, paving the way for further advancements in human-machine interaction and gesture-controlled robotics.

Kumar et al. (2012) [9], introduced the DG5 VHand 2.0, a wireless data glove designed to enhance gesture-based interactions in virtual and augmented reality. The glove featured flex sensors, which captured precise finger movements, and Bluetooth connectivity, ensuring seamless real-time communication with external devices. To process gesture inputs effectively, the researchers employed a K-Nearest Neighbors (K-NN) classifier, allowing their system to recognize complex hand motions with high accuracy. The glove was particularly effective for applications such as air-writing, where users could trace letters mid-air, and 3D sketching, enabling intuitive and immersive design processes within virtual environments. Their findings underscored the potential of wearable sensor technology in expanding the boundaries of human-computer interaction, demonstrating how real-time gesture recognition could improve accessibility, user experience, and creative workflows in emerging technologies.

Sturman and Zeltzer (1994) [10],  conducted an extensive survey of early glove-based input devices, highlighting the technological innovations that laid the foundation for modern hand-tracking systems. Among these, the VPL DataGlove utilized optical fibers to measure finger flexion, providing a level of precision that was groundbreaking at the time. Meanwhile, the Dexterous HandMaster (DHM) relied on Hall-effect sensors, which detected changes in magnetic fields to determine finger movements. These devices represented significant strides in human-computer interaction, offering high-accuracy motion tracking crucial for applications in virtual reality (VR) and robotic control.

Huang (2017) [11], explored the application of the MPU6050 sensor in flight control systems, utilizing its integrated accelerometers and gyroscopes to detect tilt angles with high accuracy. This sensor's ability to measure both linear acceleration and angular velocity made it ideal for stabilizing aircraft movements and ensuring precise orientation tracking.
